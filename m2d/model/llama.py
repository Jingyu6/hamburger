import re
from enum import Enum
from typing import Dict, List, Optional

import lightning as L
import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers.modeling_flash_attention_utils as utils
from transformers import AutoTokenizer, LlamaForCausalLM
from transformers.cache_utils import DynamicCache
from transformers.modeling_outputs import BaseModelOutputWithPast

from m2d.config import GenConfig
from m2d.model.fa2_monkey_patch import prepare_fa2_from_position_ids
from m2d.model.m2d_modules import (CompositionalEmbedder,
                                   ConditionalMicroStepDecoder)
from m2d.model.teacher import DistillTeacher

# apply a monkey patch here
utils.prepare_fa2_from_position_ids = prepare_fa2_from_position_ids


class SpeedupReport:
    def __init__(self):
        self.total_queries = 0
        self.speedup = 0
        self.reset()

    def add_query_stats(self, macro, micro):
        self.total_queries += 1
        self.speedup += (1.0 * micro / macro)
    
    def reset(self):
        self.total_queries = 0
        self.speedup = 0

    def get_speedup(self):
        if self.total_queries == 0:
            print("No records yet.")
            return None
        avg_speedup = self.speedup / self.total_queries * 100
        print(f"Total number of queries: {self.total_queries}. Avg speedup: {avg_speedup:.2f}%")
        return avg_speedup


class MergeMode(Enum):
    # this is usually used for prefill
    PREFILL_NO_MERGE = 1
    # this is usually used for normal decoding
    DECODE_MERGE = 2
    # this is often used for speculative decoding where
    # the last token is generated by the target model and hence
    # need to be seperated out
    DECODE_SEP_LAST = 3


class M2DLlama(L.LightningModule):
    def __init__(
        self, 
        base_model_name: str = "meta-llama/Llama-3.2-1B-Instruct", 
        max_steps: int = 4, 
        distill_kl: Optional[float] = None
    ):
        super().__init__()
        self.save_hyperparameters()
        self.base_model_name = base_model_name
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True) # for generation

        # this is for optimization
        self.model: LlamaForCausalLM = LlamaForCausalLM.from_pretrained(
            base_model_name, 
            torch_dtype=torch.bfloat16, 
            attn_implementation="flash_attention_2",
            # device_map="auto"
        )
        self.comp_embedder = CompositionalEmbedder(
            embedding=self.model.model.embed_tokens, 
            max_steps=max_steps
        )
        # create conditional micro step decoder
        self.micro_step_decoder = ConditionalMicroStepDecoder(
            config=self.model.config, 
            max_steps=max_steps
        )
        self.max_steps = max_steps
        self.train()

        self.distill_kl = distill_kl
        if distill_kl is not None:
            self.teacher = DistillTeacher(teacher_model_name=base_model_name)
            self.kl_loss = nn.KLDivLoss(reduction="none")

        self.report = SpeedupReport()

    @torch.inference_mode
    def _macro_step(
        self, 
        prefix_ids: torch.Tensor, 
        prefix_len: int, 
        merge_mode: MergeMode, 
        macro_past_key_values: DynamicCache, 
        config: GenConfig
    ):
        if merge_mode != MergeMode.DECODE_SEP_LAST or len(prefix_ids) == 1:
            token_embeds = self.comp_embedder.single_forward(
                input_ids=prefix_ids, 
                disable_merge=(merge_mode == MergeMode.PREFILL_NO_MERGE)
            )[None, ]
            position_ids = torch.arange(0, token_embeds.shape[1], device=self.model.device)[None, ]
        else:
            front_embeds = self.comp_embedder.single_forward(
                input_ids=prefix_ids[:-1], 
                disable_merge=False
            )[None, ]
            last_embeds = self.comp_embedder.single_forward(
                input_ids=prefix_ids[-1:], 
                disable_merge=True
            )[None, ]
            token_embeds = torch.concat([front_embeds, last_embeds], dim=1)
            position_ids = torch.LongTensor([0, front_embeds.shape[1]]).to(self.model.device)[None, ]

        if merge_mode != MergeMode.PREFILL_NO_MERGE:
            # correct position ids
            position_ids += prefix_len

        base_output: BaseModelOutputWithPast = self.model.model.forward(
            inputs_embeds=token_embeds, 
            position_ids=position_ids, 
            use_cache=True, 
            output_hidden_states=True, 
            return_dict=True, 
            past_key_values=macro_past_key_values
        )

        # always takes the last one
        hidden_states = torch.concat([
            base_output.hidden_states[layer_idx + 1][:, -1:, :]
            for layer_idx in self.micro_step_decoder.feature_layer_indices
        ], dim=1)

        # MICRO STEP
        # TODO: refactor this to encapsulate everything
        micro_past_key_values = DynamicCache()
        hiddens = hidden_states
        output_ids = []
        output_token_logits = []
        output_token_probs = []

        for micro_idx in range(self.max_steps):
            if micro_idx > 0:
                past_seen_tokens = micro_past_key_values.get_seq_length()
                cache_position = torch.arange(
                    past_seen_tokens, past_seen_tokens + hiddens.shape[1], device=hiddens.device
                )
                position_ids = cache_position.unsqueeze(0)

                for decoder_layer in self.micro_step_decoder.decoders:
                    position_embeddings = self.micro_step_decoder.rotary_emb(
                        hiddens, 
                        position_ids
                    )

                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                        hiddens = decoder_layer.forward(
                            hiddens, 
                            use_cache=True, 
                            past_key_value=micro_past_key_values,
                            cache_position=cache_position,  
                            position_embeddings=position_embeddings, 
                        )[0]
            
            stops = self.micro_step_decoder.stop_head.forward(hiddens[:, -1:, :])
            pred_stop = F.softmax(stops, dim=-1).view(-1)
            logits = self.model.lm_head.forward(hiddens[:, -1:, :])
            pred_token = logits.argmax(dim=-1).view(-1)

            prob = self._get_prob(logits).item()
            
            # update hidden
            if micro_idx > 0:
                hiddens = self.comp_embedder.embedding.forward(
                    pred_token
                )[None, ]
            else:
                hiddens = torch.concat([
                    hiddens, 
                    self.comp_embedder.embedding.forward(pred_token)[None, ]
                ], dim=1)

            # update next macro step values
            output_ids.append(pred_token)
            output_token_logits.append(logits)
            output_token_probs.append(prob)

            if config.micro_step_confidence is not None:
                if pred_stop[0] < config.micro_step_confidence:
                    break
            elif pred_stop.argmax(0) == 1:
                break

        return output_ids, output_token_probs, output_token_logits

    @torch.inference_mode
    def speculate(
        self, 
        input_ids: torch.Tensor,  
        past_key_values: Optional[DynamicCache] = None, 
        prefix_len: int = 0, 
        merge_mode: MergeMode = MergeMode.PREFILL_NO_MERGE, 
        config: Optional[GenConfig] = None, 
        **kwargs
    ):
        self.eval()

        if config is None:
            config = GenConfig()

        if past_key_values is None:
            macro_past_key_values = DynamicCache()
        else:
            macro_past_key_values = past_key_values

        if merge_mode != MergeMode.PREFILL_NO_MERGE:
            assert prefix_len > 0
        
        assert input_ids.shape[0] == 1, "Currently only support batch size = 1."

        ids, _, logits = self._macro_step(
            prefix_ids=input_ids.view(-1), 
            prefix_len=prefix_len, 
            merge_mode=merge_mode, 
            macro_past_key_values=macro_past_key_values, 
            config=config
        )

        return {
            "ids": torch.stack(ids, dim=-1), 
            "logits": torch.concat(logits, dim=1), 
            "past_key_values": macro_past_key_values
        }

    @torch.inference_mode
    def generate(
        self, 
        prompt: Optional[str] = None, 
        conversation: Optional[List[Dict]] = None, 
        config: Optional[GenConfig] = None
    ) -> str:
        self.eval()

        if config is None:
            config = GenConfig()

        if prompt is not None:
            conversation = [{"role": "user", "content": prompt}]
        else:
            assert conversation is not None
        
        if config.system_message is not None:
            if conversation[0]["role"] == "system":
                print("Warning: Input already has a system message while attemping to add another one.")
            conversation = [{"role": "system", "content": config.system_message}] + conversation

        input_ids = self.tokenizer.apply_chat_template(
            conversation, 
            add_generation_prompt=True, 
            return_tensors='pt', 
            return_dict=True
        )["input_ids"][0].to(self.model.device)
        
        # create a cache object
        macro_past_key_values = DynamicCache()

        seq_len = input_ids.shape[-1]
        # TODO: refactor later
        history_ids = input_ids.clone()
        output_token_ids = []
        output_token_probs = []
        total_len = seq_len

        # MACRO STEP
        for macro_idx in range(config.decode_steps):
            ids, probs, _ = self._macro_step(
                prefix_ids=input_ids, 
                prefix_len=total_len, 
                merge_mode=MergeMode.PREFILL_NO_MERGE if macro_idx == 0 else MergeMode.DECODE_MERGE, 
                macro_past_key_values=macro_past_key_values, 
                config=config
            )

            input_ids = torch.concat(ids).flatten()
            total_len += len(input_ids)
            output_token_ids.append(input_ids)
            history_ids = torch.concat([history_ids, input_ids], dim=-1)
            output_token_probs.extend(probs)

            if any(input_ids == self.tokenizer.eos_token_id):
                break

            if (total_len - seq_len) >= config.max_gen_len:
                break
        
        all_token_ids = torch.concat(output_token_ids, dim=0).cpu()
        output = self.tokenizer.decode(all_token_ids, skip_special_tokens=True)
        token_str_list = self.tokenizer.batch_decode(all_token_ids.view(-1))
        token_output = "\033[42m \033[0m".join(self._color_output(
            token_str_list, 
            output_token_probs, 
            [len(x) for x in output_token_ids]
        ))

        if config.remove_think:
            # remove the think block in the output
            output = re.sub(r"<think>.*?</think>[\s\r\n]*", "", output, flags=re.DOTALL)

        self.report.add_query_stats(
            macro=len(output_token_ids), 
            micro=all_token_ids.shape[0]
        )

        return {
            "output": output, 
            "micro_token_output": token_output, 
            "speedup": all_token_ids.shape[0] / len(output_token_ids)
        }
    
    def _color_output(self, token_list, prob_list, steps):
        colors = [None, 231, 159, 51, 48]
        colored_token_list = []
        for token, prob in zip(token_list, prob_list):
            color_idx = min(int(prob / 0.2), 4)
            color = colors[color_idx]
            if color is None:
                colored_token_list.append(token)
            else:
                colored_token_list.append(f"\033[38;5;{color}m{token}\033[0m")
        
        concat_colored_token_list = []
        idx = 0
        for step in steps:
            concat_colored_token_list.append("".join(colored_token_list[idx:idx + step]))
            idx += step

        return concat_colored_token_list
    
    def _get_prob(self, logits: torch.Tensor, token_id: Optional[torch.Tensor] = None):
        logits = logits.view(-1)
        if token_id is None:
            token_id = logits.argmax(dim=-1, keepdim=True)
        probs = F.softmax(logits, dim=-1)
        return probs[token_id]

    def forward(
        self, 
        input_ids: torch.LongTensor, 
        seq_lens: List[int], 
        inst_lens: List[int], 
        steps: List[List[int]] 
    ):
        # composition embedding
        token_embeds, position_ids, comp_seq_lens, unmerged_embeds \
            = self.comp_embedder.forward(
                input_ids, 
                seq_lens, 
                inst_lens, 
                steps, 
                return_unmerged=True
            )

        # get hidden state of the base llama
        base_output: BaseModelOutputWithPast = self.model.model.forward(
            inputs_embeds=token_embeds, 
            position_ids=position_ids, 
            use_cache=False, 
            output_hidden_states=True, 
            return_dict=True, 
        )

        # take off the embeddings, number of layers
        hidden_states = base_output.hidden_states[1:]

        # micro step decoding [num_of_decodes, max_steps, model_size]
        micro_step_outputs, stop_outputs = self.micro_step_decoder.forward(
            hidden_states=hidden_states, 
            token_embeds=unmerged_embeds, # not sure if we want to use detach here
            comp_seq_lens=comp_seq_lens, 
            inst_lens=inst_lens
        )

        assert self.model.config.pretraining_tp == 1

        # calculate logits
        logits = self.model.lm_head.forward(self._trim_micro_steps(micro_step_outputs, steps))
        stops = self._trim_micro_steps(stop_outputs, steps)

        return logits, stops

    def _trim_micro_steps(
        self,
        data: torch.Tensor, 
        steps: List[List[int]]
    ) -> torch.Tensor:
        """
            We remove unnecessary tokens here
            e.g.: 
            [---max_step---]
            [t0][t1][t2][pd][pd]
            [m0][m1][m2][m3][m4]
            
            [t0][t1][t2]
            [m0][m1][m2]
            This will improve loss accuracy and memory usage
        """
        step = []
        for s in steps:
            step.extend(s)
        trim_data = []
        for s, d in zip(step, data):
            trim_data.append(d[:s])
        return torch.concat(trim_data, dim=0)

    def _get_targets(
        self,
        input_ids: torch.LongTensor, 
        seq_lens: List[int], 
        inst_lens: List[int], 
        steps: List[List[int]] 
    ):
        label_targets = []
        stop_positions = []
        offset = 0
        for seq_len, inst_len, step in zip(seq_lens, inst_lens, steps):
            label_targets.extend(
                input_ids[offset + inst_len:offset + seq_len].split(
                    split_size=step, 
                    dim=0
                )
            )
            stop_positions.extend(step)
            offset += seq_len
        
        label_targets = torch.concat(label_targets, dim=0)
        stop_positions = torch.LongTensor(stop_positions).to(label_targets.device)
        stop_positions = torch.cumsum(stop_positions, dim=0) - 1
        stop_targets = torch.zeros_like(label_targets)
        stop_targets.scatter_(dim=0, index=stop_positions, value=1)

        return label_targets, stop_targets

    def _calc_loss(
        self, 
        values: torch.Tensor, 
        targets: torch.Tensor, 
        loss_type: str = "ce"
    ):
        if loss_type == "ce":
            loss_fct = torch.nn.CrossEntropyLoss()
        elif loss_type == "bce":
            loss_fct = torch.nn.BCEWithLogitsLoss()
            targets = targets.float().unsqueeze(-1)

        loss = loss_fct(values.float(), targets)
        return loss

    def _calc_accuracy(
        self, 
        logits: torch.Tensor, 
        stops: torch.Tensor, 
        label_targets: torch.Tensor, 
        stop_targets: torch.Tensor, 
        steps: List[List[int]]
    ):
        token_pred = logits.argmax(dim=-1)
        token_correct_mask = (token_pred == label_targets)
        token_acc = torch.sum(token_correct_mask) / len(label_targets.view(-1))

        stop_correct_mask = ((stops.view(-1) > 0) == (stop_targets > 0.5))
        stop_acc = torch.sum(stop_correct_mask) / len(stop_targets.view(-1))

        metrics = {
            "eval_token_acc": token_acc, 
            "eval_stop_acc": stop_acc
        }

        flattened_steps = [0]
        for step in steps:
            flattened_steps.extend(step)
        flattened_steps = torch.cumsum(
            torch.LongTensor(flattened_steps).to(token_correct_mask.device), dim=0)

        def _calc_step_mask(step):
            mask = torch.zeros_like(token_correct_mask, dtype=torch.bool)
            for i in range(len(flattened_steps) - 1):
                start = flattened_steps[i]
                end = flattened_steps[i + 1]
                pos = start + step
                if pos < end:
                    mask[pos] = True
            return mask

        # # per-step metrics
        for step in range(self.max_steps):
            step_mask = _calc_step_mask(step)
            acc_step = torch.sum(token_correct_mask & step_mask) / (torch.sum(step_mask) + 1e-9)
            metrics[f"eval_token_acc_s{step + 1}"] = acc_step

        return metrics

    def training_step(self, batch, batch_idx):
        """
        batch: {
            "input_ids": input_ids, 
            "seq_lens": seq_lens, 
            "inst_lens": inst_lens, 
            "steps": steps
        }
        """
        logits, stops = self.forward(**batch)
        label_targets, stop_targets = self._get_targets(**batch)
        label_loss = self._calc_loss(logits, label_targets)
        stop_loss = self._calc_loss(stops, stop_targets, loss_type='bce')
        loss = label_loss + stop_loss * 0.5

        log_dict = {
            "train_loss": label_loss, 
            "train_perplexity": torch.exp(label_loss)
        }

        if self.distill_kl is not None:
            with torch.no_grad():
                teacher_logits, mask = self.teacher.get_logits(**batch)

            # put into the correct space
            teacher_logits = F.softmax(teacher_logits, dim=-1)
            logits = F.log_softmax(logits, dim=-1)

            kl_loss = self.kl_loss(logits, teacher_logits)

            # apply mask and avg
            kl_loss = (kl_loss * mask.unsqueeze(-1)).sum() / mask.sum()

            loss += self.distill_kl * kl_loss

            log_dict["kl_loss"] = kl_loss

        self.log_dict(log_dict, on_step=True, prog_bar=True, logger=True)

        return loss

    def validation_step(self, batch, batch_idx):
        logits, stops = self.forward(**batch)
        label_targets, stop_targets = self._get_targets(**batch)
        label_loss = self._calc_loss(logits, label_targets)

        log_dict = {
            "eval_loss": label_loss, 
            "eval_perplexity": torch.exp(label_loss)
        }

        log_dict.update(self._calc_accuracy(
            logits, 
            stops, 
            label_targets, 
            stop_targets, 
            batch["steps"]))

        self.log_dict(
            log_dict, 
            prog_bar=True, 
            logger=True, 
            sync_dist=True, 
            batch_size=len(batch["seq_lens"])
        )

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW([
                # smaller learning rate for the main model
                {"params": self.model.model.embed_tokens.parameters(), "lr": 5e-5}, 
                # slightly larger lr for the embedding and lm head (tied)
                {"params": [p for n, p in self.model.named_parameters() if "embed_tokens" not in n], "lr": 5e-5}, 
                # larger lr for grafted modules
                {"params": self.comp_embedder.merger.parameters()}, 
                {"params": self.comp_embedder.out_proj.parameters()}, 
                {"params": self.micro_step_decoder.parameters()}, 
            ], lr=1e-4
        )

        total_steps = self.trainer.estimated_stepping_batches
        warmup_steps = int(0.0 * total_steps)
        stable_steps = int(0.7 * total_steps)
        decay_steps = total_steps - warmup_steps - stable_steps

        def lr_lambda_wsd(step):
            if step < warmup_steps:
                return step / warmup_steps
            elif step < (warmup_steps + stable_steps):
                return 1.0
            else:
                return (total_steps - step) / decay_steps
        
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda_wsd)

        return {
            "optimizer": optimizer, 
            "lr_scheduler": {
                "scheduler": scheduler, 
                "interval": "step"
            }
        }


if __name__ == "__main__":
    model = M2DLlama()
